<p align="center">
  <img src="banner.png" alt="Deep Learning from Scratch â€” by Pierre Chambet" width="800">
</p>

<h1 align="center">Deep Learning from Scratch</h1>
<p align="center">
  From first principles to real images â€” one neuron, one layer, one insight at a time.<br>
  <a href="https://www.linkedin.com/in/pierre-chambet-289a5b220/">LinkedIn</a> â€¢ 
  <a href="https://github.com/Pchambet">GitHub</a>
</p>

---

> â€œDonâ€™t just run `.fit()`. Build the thing, understand it, and then trust it.â€

---

## ğŸ‘‹ About This Project

Iâ€™m **Pierre Chambet**, a data & deep learning student-engineer who decided to rebuild Deep Learning from scratch â€”  
not by copying frameworks, but by *understanding every equation, line, and gradient*.

This repository is my **learning-in-public laboratory**.  
It documents the full path from a **hand-coded neuron** in NumPy to a **convolutional network** on MNIST â€”  
all explained, derived, and visualized with care.

Itâ€™s both a **portfolio of understanding** and a **teaching resource**:  
math â†’ code â†’ intuition â†’ result.

---

## ğŸ§­ Project Architecture

| Layer | Content | Purpose |
|--------|----------|----------|
| **PDF Guides** | `main_capstone.pdf`, `mnist_guide.pdf`, `cnn_guide.pdf` | Theoretical backbone and narrative |
| **Notebooks (01â€“12)** | Full implementations, from neuron â†’ CNN | Code + visual demonstrations |
| **LinkedIn Series** | Weekly public lessons | Outreach, credibility, reflection |

> The PDFs tell the *why*, the notebooks show the *how*,  
> and the posts share the *journey*.

---

## ğŸ§© Notebook Index (Chronological Path)

| # | Notebook | Focus | Output |
|:-:|-----------|--------|---------|
| 01 | **Single Neuron** | Linear model, sigmoid activation | Decision boundary |
| 02 | **Log-Loss & Metrics** | Binary cross-entropy, clipping, accuracy | Loss curve |
| 03 | **Gradients by Hand** | âˆ‚L/âˆ‚w and âˆ‚L/âˆ‚b derivation | Gradient sanity check |
| 04 | **Training Loop** | Forward â†’ loss â†’ backward â†’ update | Accuracy over time |
| 05 | **Image Pipeline** | Load & normalize data (HDF5 or MNIST) | Sample grid |
| 06 | **From Scratch on Images** | Apply hand-built loop to real pixels | Training curve |
| 07 | **Two-Layer Gradients** | Derive and visualize 2-layer backprop | Equations & schema |
| 08 | **Two-Layer Network** | Implement full 2-layer NN | Non-linear boundary |
| 09 | **Backprop Any Depth** | General L-layer backprop (looped) | Gradient flow |
| 10 | **Decision Boundaries** | Moons / Circles / Blobs | Boundary comparison |
| 11 | **MNIST MLP Baseline** | Dense network + error analysis | Confusion matrix |
| 12 | **MNIST CNN Baseline** | Convolutional net + feature maps | Learned filters |

---

## ğŸ“˜ Guides (Theory PDFs)

| File | Theme | Role |
|------|--------|------|
| `main_capstone.pdf` | **Fundamentals & Training Logic** | The full story â€” neurons, gradients, learning loop |
| `mnist_guide.pdf` | **Dense Networks on MNIST** | How to move from vectors to real handwritten digits |
| `cnn_guide.pdf` | **Understanding Convolutions** | Why spatial structure changes everything |

> These PDFs are not static papers â€” they mirror the notebooks and serve as theoretical anchors.

---

## âš™ï¸ Quickstart

```bash
git clone https://github.com/Pchambet/deep-learning-from-scratch.git
cd deep-learning-from-scratch
python -m venv .venv && source .venv/bin/activate
pip install -r env/requirements.txt
jupyter lab notebooks/01_single_neuron.ipynb
```

---

## ğŸ§± Repository Structure

```
deep-learning-from-scratch/
â”œâ”€â”€ notebooks/           # 01â€“12 notebooks (chronological learning path)
â”œâ”€â”€ pdf/                 # main_capstone.pdf, mnist_guide.pdf, cnn_guide.pdf
â”œâ”€â”€ src/                 # helper code (e.g., utilities.py)
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ figures/         # exported plots (decision boundaries, confusion matrices)
â”‚   â””â”€â”€ banners/         # repo and LinkedIn visuals
â”œâ”€â”€ env/                 # requirements and environment files
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ“¢ LinkedIn Series â€” #DeepLearningJourney

Every notebook becomes a short, visual lesson shared on [**LinkedIn**](https://www.linkedin.com/in/pierre-chambet-289a5b220/).  
Each post includes 1 idea, 1 plot, and 1 link to the corresponding notebook.

| Episode | Title | Notebook |
|:--------|:-------|:----------|
| 1 | *I built a neuron from scratch* | [`01_single_neuron.ipynb`](notebooks/01_single_neuron.ipynb) |
| 2 | *Log-loss explained in 60 seconds* | [`02_logloss_and_metrics.ipynb`](notebooks/02_logloss_and_metrics.ipynb) |
| 3 | *How backprop really works* | [`03_gradients_single_neuron.ipynb`](notebooks/03_gradients_single_neuron.ipynb) |
| 4 | *A training loop that actually learns* | [`04_training_loop_from_scratch.ipynb`](notebooks/04_training_loop_from_scratch.ipynb) |
| 5 | *From vectors to images â€” MNIST* | [`11_mnist_mlp_baseline.ipynb`](notebooks/11_mnist_mlp_baseline.ipynb) |
| 6 | *When the network starts to see â€” CNNs* | [`12_mnist_cnn_baseline.ipynb`](notebooks/12_mnist_cnn_baseline.ipynb) |
| 7 | *The big picture: from neuron to CNN* | [`pdf/main_capstone.pdf`](pdf/main_capstone.pdf) |

---

## ğŸ§  Philosophy

> â€œLearning isnâ€™t remembering â€” itâ€™s rebuilding.â€

No shortcuts, no black boxes.  
Every weight, bias, and gradient is traced.  
This is **real deep learning** â€” in both name and process.

---

## ğŸ§¾ For Recruiters

**In five minutes**, this repo tells you that I:
- Understand the math behind neural networks.  
- Can implement and debug deep learning models end-to-end.  
- Communicate complex ideas clearly and visually.  
- Learn independently, structure work, and deliver clean results.

Start with:
- `01_single_neuron.ipynb` (clarity)
- `04_training_loop_from_scratch.ipynb` (method)
- `11_mnist_mlp_baseline.ipynb` (application)
- `12_mnist_cnn_baseline.ipynb` (maturity)

---

## ğŸ¤ Contribute / Connect

If you find an error or idea worth exploring, open an issue or PR.  
If youâ€™re learning in public too, tag me â€” letâ€™s connect.

<p align="center">
  <a href="https://www.linkedin.com/in/pierre-chambet-289a5b220/">
    <img src="https://img.shields.io/badge/Follow%20on%20LinkedIn-blue?style=flat-square&logo=linkedin" alt="LinkedIn">
  </a>
  <a href="https://github.com/Pchambet">
    <img src="https://img.shields.io/badge/Explore%20more%20projects-black?style=flat-square&logo=github" alt="GitHub">
  </a>
</p>

---

<p align="center"><i>
Deep Learning from Scratch â€” built with patience, mathematics, and curiosity.<br>
Â© 2025 Pierre Chambet. All rights reserved.
</i></p>
