<p align="center">
  <img src="banner.png" alt="Deep Learning Journey — by Pierre Chambet" width="800">
</p>

<h1 align="center">Deep Learning Journey</h1>
<p align="center">
  From neuron to CNN, built and explained from scratch.<br>
  <a href="https://www.linkedin.com/in/pierre-chambet-289a5b220/">LinkedIn</a> • 
  <a href="https://github.com/Pchambet">GitHub</a>
</p>

---

> “Each week, I take one Deep Learning concept and rebuild it from the ground up — math, intuition, and code — until it makes sense.”

I’m Pierre, a data & deep learning student-engineer who wanted to **understand, not just use** AI.  
This repository is my open notebook — a transparent journey from the simplest neuron to full convolutional networks,  
where each step is documented, coded, and shared.

This is not a bootcamp recap or a course copy.  
It’s **a reconstruction of Deep Learning from first principles**, in the voice of someone who learns by building.

---

## Why This Project Exists

Because I was tired of calling `.fit()` without knowing what really happened underneath.  
I wanted to **connect the dots** between calculus, code, and intuition — to *see* backpropagation unfold, to *feel* why activations matter, and to *believe* in the math driving it all.

So I built my own curriculum — from scratch, for real.

> This project is both a **learning series** and a **public notebook**.  
> Each guide is a self-contained step of the journey, both theoretical and practical.

---

## The Series Structure

| Step | Theme | Notebook(s) | Description |
|:----:|:------|:-------------|:-------------|
| 0 | **Before You Start** | [`read_this_before_and_everyday.ipynb`](read_this_before_and_everyday.ipynb) | How this journey is structured and how to get the most out of it. |
| 1 | **Neuron & Activation** | [`theory_00.ipynb`](theory_00.ipynb) • [`practice_00.ipynb`](practice_00.ipynb) | What is a neuron? Linear model, sigmoid, gradient derivation, first forward/backward propagation. |
| 2 | **From Math to Code** | [`guide_01_companion.ipynb`](guide_01_companion.ipynb) • [`practice_01.ipynb`](practice_01.ipynb) | Full implementation of a simple neural net. Logic, equations, and NumPy code converge. |
| 3 | **Deep Networks** | [`theory_02.ipynb`](theory_02.ipynb) • [`practice_02.ipynb`](practice_02.ipynb) • [`practice_03.ipynb`](practice_03.ipynb) | Generalization to multiple layers. Matrix calculus for full backpropagation. |
| 4 | **From Pixels to Predictions (MNIST)** | [`mnist.ipynb`](mnist.ipynb) | Apply our from-scratch model to MNIST — preprocessing, training, and error analysis. |
| 5 | **Seeing with CNNs** | [`CNN.ipynb`](CNN.ipynb) | Move from flattened pixels to structured images. Learn locality, parameter sharing, and invariance. |
| 6 | **Meta Reflection** | [`main.pdf`](main.pdf) | The full story: lessons learned, synthesis, and philosophy behind the series. |

---

## How to Navigate

1. **Start from Step 0** — it gives you the mindset and the rules of the game.  
2. **Alternate between theory and practice** — each concept is followed by its live implementation.  
3. **Read, code, question.** Don’t skip the “why”.  
4. **Check the PDF guides** for the narrative explanations.  
5. **Compare the models** (MLP vs CNN) — that’s where understanding becomes insight.

> Every notebook stands on its own, but all together, they form one continuous arc —  
> from *mathematical curiosity* to *computational mastery.*

---

## LinkedIn Series

This journey is also being shared as a public learning series on LinkedIn.  
Each post summarizes a notebook, shares visuals, and reflects on what was learned.

Follow along → [**#DeepLearningJourney**](https://www.linkedin.com/in/pierre-chambet-289a5b220/)

| Episode | Title | Notebook Link |
|:--------|:-------|:---------------|
| 1 | *What’s really happening inside a neuron?* | [`theory_00.ipynb`](theory_00.ipynb) |
| 2 | *From math to code: my first neural net from scratch* | [`practice_01.ipynb`](practice_01.ipynb) |
| 3 | *From one neuron to an entire brain* | [`practice_02.ipynb`](practice_02.ipynb) |
| 4 | *Can a network see? Understanding MNIST* | [`mnist.ipynb`](mnist.ipynb) |
| 5 | *When the network starts to see shapes — CNNs* | [`CNN.ipynb`](CNN.ipynb) |

---

## Tech Stack

- **Language:** Python 3 + NumPy / PyTorch / TensorFlow  
- **Environment:** Jupyter Notebooks  
- **Math:** Linear Algebra, Calculus, Optimization  
- **Visualization:** Matplotlib, Seaborn  

---

## Philosophy

> “Learning isn’t remembering — it’s rebuilding.”

I don’t aim to be fast.  
I aim to be *clear*.  
Every equation is derived. Every plot is explained.  
This is learning **by reconstruction** — one concept, one notebook, one insight at a time.

---

## Contribute / Connect

If you find an error, open an issue or PR.  
If you’re also on a learning journey, tag me on LinkedIn — I’d love to see your version.

<p align="center">
  <a href="https://www.linkedin.com/in/pierre-chambet-289a5b220/">
    <img src="https://img.shields.io/badge/Follow%20on%20LinkedIn-blue?style=flat-square&logo=linkedin" alt="LinkedIn">
  </a>
  <a href="https://github.com/Pchambet">
    <img src="https://img.shields.io/badge/Explore%20more%20projects-black?style=flat-square&logo=github" alt="GitHub">
  </a>
</p>

---

<p align="center"><i>
Deep Learning Journey — built with patience, mathematics, and curiosity.<br>
© 2025 Pierre Chambet. All rights reserved.
</i></p>
