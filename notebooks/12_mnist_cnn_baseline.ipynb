{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc991fa",
   "metadata": {},
   "source": [
    "## From Vectors to Vision: Introducing Convolutional Neural Networks\n",
    "\n",
    "Our previous model — a Multi-Layer Perceptron — treated each image as a flat vector of 784 numbers.\n",
    "\n",
    "But... does that make sense?\n",
    "\n",
    "When humans look at a digit, we don't scan each pixel one-by-one.  \n",
    "We recognize *shapes*, *edges*, *patterns*.  \n",
    "A \"9\" looks like a loop with a stick. A \"3\" has two bumps.  \n",
    "And those patterns live in the **spatial structure** of the image.\n",
    "\n",
    "A Convolutional Neural Network (CNN) captures that spatial information.  \n",
    "It doesn't flatten. It slides filters across the image, detecting local patterns like edges, curves, and corners.\n",
    "\n",
    "Let’s now build a CNN — a model that *sees* like a machine, but learns like us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f95f27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8b882",
   "metadata": {},
   "source": [
    "But here, we have a problem. The data as there are now are just arrays of numbers, matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649ae09",
   "metadata": {},
   "source": [
    "deja, comme d'hab, on normalise les donnes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7cbf1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62032de",
   "metadata": {},
   "source": [
    "## ajouter le 4eme dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95679d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test  = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9cc2ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c1bad",
   "metadata": {},
   "source": [
    "maintenant, chaque image est represntee comme une plaque d'epaisseur 1, de taille 28x28. il y a donc 60000 plaques d'epaisseur 1, de taille 28x28."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb169a60",
   "metadata": {},
   "source": [
    "pour les labels, meme histoire, on les transforme en vecteurs one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13158545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
